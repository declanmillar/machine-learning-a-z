{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "\n",
    "## Bayes' Theorem Intuition\n",
    "\n",
    "Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. The formula is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    P(A|B) = \\frac{P(B|A) P(A)}{P(B)}.\n",
    "\\end{equation}\n",
    "\n",
    "In words, the probability of A (happening) given B, is given by the probability of B given A, multiplied by the probability of A, divided by the probability of B (see Appendix A for the derivation).\n",
    "\n",
    "As an example, imagine you have two spanner-making machines, $m_1$ and $m_2$, producing:\n",
    "\n",
    "\\begin{equation}\n",
    "    m_1: 30\\,\\mathrm{spanners/hr}\\\\\n",
    "    m_2: 20\\,\\mathrm{spanners/hr}\n",
    "\\end{equation}\n",
    "\n",
    "Of all the produced spanners, we see that 1\\% are defective, with 50% labelled with $m_1$ and 50% with $m_2$. What is the probability that that a part produced by $m_2$ is defective?\n",
    "\n",
    "Firstly, to get this problem in useful notation, the probabilties of an individual spanner coming from either machine are:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(m_1) = 30/50 = 0.6\\\\\n",
    "    P(m_2) = 20/50 = 0.4\n",
    "\\end{equation}\n",
    "\n",
    "And the overall probability of defect is\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\mathrm{X}): 30/50 = 0.6\n",
    "\\end{equation}\n",
    "\n",
    "We also know that the probability of a single spanner being defective given it came from a particular machine is\n",
    "\n",
    "\\begin{equation}\n",
    "    P(m_1 | \\mathrm{X}) = 0.5,\\\\\n",
    "    P(m_2 | \\mathrm{X}) = 0.5.\n",
    "\\end{equation}\n",
    "\n",
    "Bayes' theorem tells us that to answer our question:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\mathrm{X} | m_2) = \\frac{P(m_2 | \\mathrm{X}) P(\\mathrm{X})}{P(m_2)}.\n",
    "\\end{equation}\n",
    "\n",
    "I.e. the probability of machine 2 producing a defective spanner is given by the probability of a defective spanner coming from machine 2, multiplied by the probability of a defective spanner being produced, divided by the probability of any spanner coming from machine 2.\n",
    "\n",
    "Substituting the information we know\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\mathrm{X} | m_2) = \\frac{0.5 \\times 0.01}{0.4} = 0.0125 = 1.25\\%.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\mathrm{X} | m_2) = \\frac{0.5 \\times 0.01}{0.6} = 0.00833 = 0.83\\%.\n",
    "\\end{equation}\n",
    "\n",
    "In a frequentist interpretation, we imagine that of 1000 spanners, 400 will be produced by machine 2. Also, 10 of the total are defective, 5 of which came from machine 2. So, the percentage of defective parts is 5/400 = 1.25\\%. So Bayes' theorem is quite intuitive.\n",
    "\n",
    "One may ask why we couldn't just count the number of defective spanners. However, perhaps the initial variables were standard factory metrics, so it's easier to just apply Bayes' theorem. In more complex applications this information must be otherwise obscured.\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "Imagine we have some labelled data of people who walk or drive to work, as well as some (assumed) independent variables about those people that we think may be relevent to this choice (features), e.g. salary and age. Now assume that a new employee is hired with a known salary and age and we want to predict whether he's going to work ($W$) or drive ($D$).\n",
    "\n",
    "![bayes1.png](bayes1.png)\n",
    "\n",
    "So, given a vector of features $X$, Bayes' theorem states\n",
    "\n",
    "\\begin{align}\n",
    "    P(W|X) &= \\frac{P(X|W) P(W)}{P(X)},\\\\\n",
    "    P(D|X) &= \\frac{P(X|D) P(D)}{P(X)}.\n",
    "\\end{align}\n",
    "\n",
    "The following terminology can be used for each variable, given in the table below, along with order in which we should find them.\n",
    "\n",
    "| Variable       | Name                 | Order to find  |\n",
    "| -------------- | -------------------- | -------------- |\n",
    "| $P(D)$         | Prior probability    | #1             |\n",
    "| $P(X)$         | Marginal likelihood  | #2             |\n",
    "| $P(X|D)$       | Likelihood           | #3             |\n",
    "| $P(D|X)$       | Posterior probabilty | #4             |\n",
    "\n",
    "We want to use this information to find $P(W|X)$ vs $P(D|X)$. Clearly, Naive Bayes' is considered a probabilistic classifier because we're first calculating the probabilites, and then, based on probabilities, we're assigning the class.\n",
    "\n",
    "Now we'll go through the steps to find a solution for this example.\n",
    "\n",
    "\n",
    "\n",
    "### Step 1 - Prior probability\n",
    "\n",
    "What's the probability that someone (anyone in the dataset) walks, regardless of independent variables. Easy, just calculate the number of walkers $N(W)$ divided by the total number of observations ($N$):\n",
    "\n",
    "\\begin{equation}\n",
    "    P(W) = \\frac{N(W)}{N} = \\frac{10}{30}\n",
    "\\end{equation}\n",
    "\n",
    "### Step 2 - Marginal likelihood\n",
    "\n",
    "To find the marginal likelihood we draw a circle of radius $R$ around the point. $R$ must be selected as an input parameter to the algorithm. We look at all the points inside the radius, which are deemed similar in terms of features to the new point.\n",
    "\n",
    "![bayes2.png](bayes2.png)\n",
    "\n",
    "So the marginal likelihood is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    P(X) = \\frac{\\mathrm{Number\\ of\\ similar\\ observations}}{\\mathrm{Total\\ observations}} = \\frac{4}{30}\n",
    "\\end{equation}\n",
    "\n",
    "This tells us the likelihood of any new random point that we add to the dataset being inside the circle.\n",
    "\n",
    "### Step 3 - Likelihood\n",
    "\n",
    "The likelihood, $P(X|W)$ uses the same circle as in the previous step, however, now we only count the number of walkers in the circle relative to the total numbe, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    P(X|W) = \\frac{\\mathrm{Number\\ of\\ similar\\ observations\\ who\\ walk}}{\\mathrm{Total\\ number\\ of\\ walkers}} = \\frac{3}{30}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Step 4 - Posterior probability\n",
    "\n",
    "Now, plugging all that in\n",
    "\n",
    "\\begin{equation}\n",
    "    P(W|X) = \\frac{P(X|W) P(W)}{P(X)} = \\frac{\\frac{3}{10}\\frac{10}{30}}{\\frac{4}{30}} = \\frac{3}{4}\n",
    "\\end{equation}\n",
    "\n",
    "So, 75\\% is the probability that someone added at this point in the feature space would be a walker.\n",
    "\n",
    "We can repeat the same steps to calculate the posterior probability for the new point at $X$ to be a driver:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(D|X) = \\frac{P(X|D) P(D)}{P(X)} = \\frac{\\frac{1}{20}\\frac{20}{30}}{\\frac{4}{30}} = \\frac{1}{4}\n",
    "\\end{equation}\n",
    "\n",
    "There's 25\\% probability that someone added at point $X$ in the feature space would be a walker.\n",
    "\n",
    "Therefore, \n",
    "\n",
    "\\begin{equation}\n",
    "    P(W|X) > P(D|X),\n",
    "\\end{equation}\n",
    "\n",
    "it's more probable that someone added with features $X$ will walk rather than drive to work.\n",
    "\n",
    "![bayes4.png](bayes4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A - Derivation of Bayes' theorem\n",
    "\n",
    "Start with the definition of conditional probability [1]. The probability of event $A$ given event $B$ is\n",
    "\n",
    "\\begin{equation}\n",
    "    P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "Likewise, the probability of event $B$ given event $A$ is\n",
    "\n",
    "\\begin{equation}\n",
    "    P(B|A) = \\frac{P(B \\cap A)}{P(A)}\n",
    "\\end{equation}\n",
    "\n",
    "We assume $P(A), P(B) \\ne 0$. Rearranging and combining these equations\n",
    "\n",
    "\\begin{equation}\n",
    "    P(A|B)P(B) = P(A \\cap B) = P(B|A)P(A)\n",
    "\\end{equation}\n",
    "\n",
    "This lemma is sometimes known as the product rule for probabilities. Assuming $P(B)\\ne 0$, we can further rearrange for Bayes' theorem,\n",
    "\n",
    "\\begin{equation}\n",
    "    P(A|B) = \\frac{P(B|A) P(A)}{P(B)}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "    [1] https://my.eng.utah.edu/~cs5961/Resources/bayes.pdf\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
