{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifical Neural Networks\n",
    "\n",
    "## The Neuron\n",
    "\n",
    "The image below shows a neuron, also known as a node.\n",
    "\n",
    "![ann1](ann1.png)\n",
    "\n",
    "The neuron takes a series of input signals and produces a single output signal. In this course, yellow nodes will signify input values, while green will signify hidden nodes and red will signify output values.\n",
    "\n",
    "<img src=\"ann2.png\" alt=\"ann2\" width=\"300\" style=\"float:right\">\n",
    "\n",
    "The values in the input layer correspond to a single observation (a single row in the database), measuring the values of multiple independent variables that have been standardized or normalized. Standardization ensures that the variables have mean of 0 and a variance of 1. In normalization you subtract the minimum value and divide by the maximum value to get values between 0 and 1. \n",
    "\n",
    "Whether you choose normalization or standardization depends on the scenario. This is necessary to ensure the network works correctly. Further reading on this can be found in Efficient BackProp by Yann LeCun et al (1998).\n",
    "\n",
    "The output value can be continuous, binary, or categorical. If the output is categorical, we can say that the neuron has multiple outputs, corresponding to the dummy variables of each category.\n",
    "\n",
    "The connecting lines between nodes in the synapse layer (dendrites) carry weights $w_{1}...w{m}$ which are adjusted as the network is trained.\n",
    "\n",
    "At the neuron, an activation function $\\phi$ is applied to the sum of weights,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi\\left(\\sum_{i=1}^{m} w_i x_i \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Depending on the function and the outcome, the signal is passed on as output to the next node.\n",
    "\n",
    "## The Activation Function\n",
    "\n",
    "The activation function can comprise a number different forms. Below, note that $x$ without a subscript indicates the sum of weights.\n",
    "\n",
    "### Threshold Function\n",
    "\n",
    "The threshold function or step function, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi(x) = \n",
    "    \\begin{cases}\n",
    "        1 \\text{ if } x \\ge 0, \\\\\n",
    "        0 \\text{ if } x < 0,\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "is a very simple function where if the value is less than 0, the function outputs 0, otherwise it's a 1,\n",
    "\n",
    "![ann3](ann3.png)\n",
    "\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "The sigmoid function or logistic function,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi(x) = \\frac{1}{1 + e^{-x}},\n",
    "\\end{equation}\n",
    "\n",
    "is smooth, asymptotically approaching $0$ below $x = 0$ and $1$ above $x = 1$. It's very useful in the final layer of the network, especially when the output is a probability.\n",
    "\n",
    "\n",
    "![ann4](ann4.png)\n",
    "\n",
    "### Rectifier Function \n",
    "\n",
    "The rectifier function,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi(x) = \\max(x,0),\n",
    "\\end{equation}\n",
    "\n",
    "is one of the most popular functions for ANNs. See Deep Sparse Rectifier Neural Networks by Xavier Glorot et al (2011) for more on why the rectifier function is so widely used.\n",
    "\n",
    "![ann5.png](ann5.png)\n",
    "\n",
    "### Hyperbolic Tangent Function (tanh)\n",
    "\n",
    "The hyperbolic tangent function,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}},\n",
    "\\end{equation}\n",
    "\n",
    "is similar to the sigmoid function but it asymtotically approaches $-1$ with an increasing negative $x$.\n",
    "\n",
    "![ann6.png](ann6.png)\n",
    "\n",
    "### Examples\n",
    "\n",
    "Assume the dependent variable is binary, $y = 0,1$. We could use a the threshold function, in which case $y=\\phi(x)$. Or we could use the sigmoid function to get the probability, $\\text{P}(y=1)=\\phi(x)$, similar to logistic regression.\n",
    "\n",
    "In neural networks, frequently the hidden layers will use rectifier functions, while the output layer will use a logistic function.\n",
    "\n",
    "![ann8.png](ann8.png)\n",
    "\n",
    "\n",
    "## How do Neural Networks Work?\n",
    "\n",
    "We're going to examine how NNs work by using a pre-trained example looking at house prices. We'll see how these are actually trained in later sections.\n",
    "\n",
    "\n",
    "We'll look at an input layer with four independent variables: area in square feet ($x_1$), numbers of bedrooms ($x_2$), distance to the city centre in miles ($x_3$), and age ($x_4$). We want to estimate the price based on these inputs.\n",
    "\n",
    "If we have no hidden layers, each input feeds in to the output layer with a given weight, and some function is applied to produce the output. For example, the function might just take the sum of these weighted values. This is analogous to multiple linear regression, where the weights correspond to simple coefficients. \n",
    "\n",
    "![ann9.png](ann9.png)\n",
    "\n",
    "However, now assume we have a pretrained neural network with a single hidden layer of five neurons. There are synaptic connections linking every node in adjacent layers. However, the weights on certain neurons will be negligible, such that, in practice, nodes in the hidden layer respond only to a subset of input variables.\n",
    "\n",
    "![ann10.png](ann10.png)\n",
    "\n",
    "For example, the first hidden neuron may only respond to $x_1$ and $x_3$, the second to $x_2$ and $x_3$, the third to $x_1$, $x_2$ and $x_4$ the fourth to all inputs and the fifth to $x_4$. These nodes have picked up on particular features of the data. We may theorize on the nature of these linked features using our own human intuition and reasoning for this particular case study.\n",
    "\n",
    "## How do Neural Networks Learn?\n",
    "\n",
    "The Neuron, in isolation, is the simplest type of neural network. It is a single layer feed-forward NN, or a perceptron. Now we speak of training, the actual measured value is denoted by $y$, and $\\hat{y}$ is the predicted value.\n",
    "\n",
    "To begin with we look at how training works for a single row of input data. We start with some initial configuration of weights and calculate $\\hat{y}$. When $\\hat{y}$ is calculated, we calculate the cost function $C(\\hat{y}, y)$. We can choose from a number of cost functions, but the most simple is half the squared difference of $y$ and $\\hat{y}$,\n",
    "\n",
    "\\begin{equation}\n",
    "    C = \\frac{1}{2}(\\hat{y} - y)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Once $C$ has been calculated, the result is fed back to adjust the weights, with the aim of minimising the cost function through successive iterations.\n",
    "\n",
    "![ann11.png](ann11.png)\n",
    "\n",
    "Now let's extend the discussion to multiple rows. We calculate $\\hat{y}$ for each row in succession. Then once all rows are calculated we calculate the cost function,\n",
    "\n",
    "\\begin{equation}\n",
    "    C = \\frac{1}{2}\\sum_i(\\hat{y}_i - y_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "This result is then fed back in to the NN to adjust the weights, i.e. all the rows share the same weights, we're not dealing with a different NN for each row.\n",
    "\n",
    "![ann12.png](ann12.png)\n",
    "\n",
    "Now we begin to reiterate the process with the adjusted weights, for all rows, until we've minimised $C$. This process is called back propagation.\n",
    "\n",
    "[Link to further reading on different cost functions](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)\n",
    "\n",
    "\n",
    "## The Curse of Dimensionality\n",
    "\n",
    "We've discussed that the result of the cost function is fed back to adjust the weights, but we haven't clarified how this is done. \n",
    "\n",
    "Let's take again the example of the simple single input perceptron.\n",
    "\n",
    "![single_input_perceptron.png](single_input_perceptron.png)\n",
    "\n",
    "Naively, we might first attempt to train it using a brute force approach. In this manner, we could try, say, 1000 different weights and find the one that minimises the cost function.\n",
    "\n",
    "![brute_force.png](brute_force.png)\n",
    "\n",
    "This might be suitable when tuning a single weight. However, as the number of weights increases, we run in to the Curse of Dimensionality. Let's look at the pre-trained example we dealt with before.\n",
    "\n",
    "![25_weights](25_weights.png)\n",
    "\n",
    "In this example there are $25$ weights, $20$ between the input and hidden layer, and $5$ between the hidden and output layer. So, if we want to test $1000$ different tunings for each weight, that's $1000^{25} = 10^{75}$ different combinations. To test a single combination requires multiple floating point operations (flop). Let's naively assume, however, that we reduce this to a single flop. The world's fastest super computer can perform at $93$ flop/s. This means that it'll take \n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{10^{75} \\text{ flop} }{ 93 \\times 10^{15} \\text{ flop/s}} \\approx 10^{58} \\text{ s} \\approx 3 \\times 10^{50} \\text{ years}\n",
    "\\end{equation}\n",
    "\n",
    "to test all combinations; that's longer than the age of the universe. So, how can we overcome this?\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The trick to overcoming the curse of dimensionality is gradient descent. Here you start at some random (or best-guess) point in $x$. You calculate the cost function at this point and differentiate this function to find the gradient of the slope. Now a point is chosen in $x$ some distance away in the direction of decreasing $C$, i.e. if the slope is negative, you next select a higher point in $x$, if positive, you select a lower point in $x$.\n",
    "\n",
    "![slope](slope.png)\n",
    "\n",
    "This process is repeated until you find a minimum in $C$.\n",
    "\n",
    "![1d_descent](1d_descent.png)\n",
    "\n",
    "In 2d this can be visualised like this.\n",
    "\n",
    "![2d_descent](2d_descent.png)\n",
    "\n",
    "And in 3d like this.\n",
    "\n",
    "![3d_descent](3d_descent.png)\n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "In the previous example, the cost function had a convex form; the only minimum was the global minimum, so whichever initial $x$ is chosen, the same local minimum will be found. However, the cost function may have a much more complex form, with multiple local minima, such as the one below.\n",
    "\n",
    "![local_minimum](local_minimum.png)\n",
    "\n",
    "In this case, gradient descent has selected a non-optimal local minimum. To deal with these functions, we can make use of stochastic gradient descent (SGD). SGD does not require a convex function to find the global minimum. How does the methodology differ?\n",
    "\n",
    "In batch gradient descent (BGD) (the first method described), the cost function is a function of all the rows, i.e. all the rows are processed each time the cost function is assessed. So, the weights are updated only after all rows are taken into account. In SGD, the cost function is evaluated on a row-by-row basis, and in a randomly chosen order; the weights are updated for each row.\n",
    "\n",
    "![batch_vs_gradient](batch_vs_gradient.png)\n",
    "\n",
    "This helps to avoid the problem where you get stuck in a local minimum, as the cost function fluctuates much more highly. It's also a much faster approach. BGD is a deterministic method, while SGD, is, naturally, stochastic.\n",
    "\n",
    "For more further reading on gradient descent methodology see [A Neural Network in 13 lines of Python (Part 2 - Gradient Descent) by Andrew Trask (2015)](https://iamtrask.github.io/2015/07/27/python-network-part2/).\n",
    "\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "In the previous sections we saw how information was entered into the input layer and propagated forward to compute $\\hat{y}$. \n",
    "\n",
    "![forward_propagation](forward_propagation.png)\n",
    "\n",
    "![back_propagation](back_propagation.png)\n",
    "\n",
    "These are then compared to the $y$ of the training set and the errors are propagated back to adjust the weights.\n",
    "\n",
    "One might think that each of the weights needs to be individually adjusted, however, due to the mathematics of backpropagation, all the weights may be adjusted at once.\n",
    "\n",
    "The mathematics of this is covered in the further reading, [Neural Networks and Deep Learning by Michael Nielson (2015)](http://neuralnetworksanddeeplearning.com/chap2.html).\n",
    "\n",
    "## Steps to training ANNs\n",
    "\n",
    "In summary the steps to train an ANN are:\n",
    "1. Randomly initialise the weights to small numbers close to 0 (but not 0).\n",
    "2. Input the first observation of your dataset in the input layer, each feature in one input node.\n",
    "3. Forward-Propagation: from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights. Propagate the activations until getting the predicted result $\\hat{y}$.\n",
    "4. Compare the predicted result $\\hat{y}$ to the actual result $y$. Measure the generated error.\n",
    "5. Backpropagation: from right to left, the error is backpropagated. Update the weights according to how much they are responsible for the error. The learning rate decides by how much we update the weights.\n",
    "6. Repeat steps 1 to 5 and update the weights after each observation (Reinforcement Learning). Or: repeat steps 1 to 5 but update the weights after a batch of observations (Batch Learning).\n",
    "7. When the whole training set has been passed through the ANN, that makes an epoch. Redo more epochs.\n",
    "\n",
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset details the customer details of a fictional bank. The bank is experiencing unusually high rate of churn (customer loss). They've taken a sample of 10,000 customers to try and diagnose the problem.\n",
    "\n",
    "We have installed Keras using\n",
    "\n",
    "```sh\n",
    "conda install -c conda-forge keras\n",
    "```\n",
    "\n",
    "Keras wraps Tensorflow and Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
