{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifical Neural Networks\n",
    "\n",
    "## The Neuron\n",
    "\n",
    "The image below shows a neuron, also known as a node.\n",
    "\n",
    "![ann1](ann1.png)\n",
    "\n",
    "The neuron takes a series of input signals and produces a single output signal. In this course, yellow nodes will signify input values, while green will signify hidden nodes and red will signify output values.\n",
    "\n",
    "<img src=\"ann2.png\" alt=\"ann2\" width=\"300\" style=\"float:right\">\n",
    "\n",
    "The values in the input layer correspond to a single observation (a single row in the database), measuring the values of multiple independent variables that have been standardized or normalized. Standardization ensures that the variables have mean of 0 and a variance of 1. In normalization you subtract the minimum value and divide by the maximum value to get values between 0 and 1. \n",
    "\n",
    "Whether you choose normalization or standardization depends on the scenario. This is necessary to ensure the network works correctly. Further reading on this can be found in Efficient BackProp by Yann LeCun et al (1998).\n",
    "\n",
    "The output value can be continuous, binary, or categorical. If the output is categorical, we can say that the neuron has multiple outputs, corresponding to the dummy variables of each category.\n",
    "\n",
    "The connecting lines between nodes in the synapse layer (dendrites) carry weights $w_{1}...w{m}$ which are adjusted as the network is trained.\n",
    "\n",
    "At the neuron, an activation function $\\phi$ is applied to the sum of weights,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi\\left(\\sum_{i=1}^{m} w_i x_i \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Depending on the function and the outcome, the signal is passed on as output to the next node.\n",
    "\n",
    "## The Activation Function\n",
    "\n",
    "The activation function can comprise a number different forms. Below, note that $x$ without a subscript indicates the sum of weights.\n",
    "\n",
    "### Threshold Function\n",
    "\n",
    "The threshold function or step function, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi(x) = \n",
    "    \\begin{cases}\n",
    "        1 \\text{ if } x \\ge 0, \\\\\n",
    "        0 \\text{ if } x < 0,\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "is a very simple function where if the value is less than 0, the function outputs 0, otherwise it's a 1,\n",
    "\n",
    "![ann3](ann3.png)\n",
    "\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "The sigmoid function or logistic function,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi(x) = \\frac{1}{1 + e^{-x}},\n",
    "\\end{equation}\n",
    "\n",
    "is smooth, asymptotically approaching $0$ below $x = 0$ and $1$ above $x = 1$. It's very useful in the final layer of the network, especially when the output is a probability.\n",
    "\n",
    "\n",
    "![ann4](ann4.png)\n",
    "\n",
    "### Rectifier Function \n",
    "\n",
    "The rectifier function,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi(x) = \\max(x,0),\n",
    "\\end{equation}\n",
    "\n",
    "is one of the most popular functions for ANNs. See Deep Sparse Rectifier Neural Networks by Xavier Glorot et al (2011) for more on why the rectifier function is so widely used.\n",
    "\n",
    "![ann5.png](ann5.png)\n",
    "\n",
    "### Hyperbolic Tangent Function (tanh)\n",
    "\n",
    "The hyperbolic tangent function,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\phi(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}},\n",
    "\\end{equation}\n",
    "\n",
    "is similar to the sigmoid function but it asymtotically approaches $-1$ with an increasing negative $x$.\n",
    "\n",
    "![ann6.png](ann6.png)\n",
    "\n",
    "### Examples\n",
    "\n",
    "Assume the dependent variable is binary, $y = 0,1$. We could use a the threshold function, in which case $y=\\phi(x)$. Or we could use the sigmoid function to get the probability, $\\text{P}(y=1)=\\phi(x)$, similar to logistic regression.\n",
    "\n",
    "In neural networks, frequently the hidden layers will use rectifier functions, while the output layer will use a logistic function.\n",
    "\n",
    "![ann8.png](ann8.png)\n",
    "\n",
    "\n",
    "## How do Neural Networks Work?\n",
    "\n",
    "We're going to examine how NNs work by using a pre-trained example looking at house prices. We'll see how these are actually trained in later sections.\n",
    "\n",
    "\n",
    "We'll look at an input layer with four independent variables: area in square feet ($x_1$), numbers of bedrooms ($x_2$), distance to the city centre in miles ($x_3$), and age ($x_4$). We want to estimate the price based on these inputs.\n",
    "\n",
    "If we have no hidden layers, each input feeds in to the output layer with a given weight, and some function is applied to produce the output. For example, the function might just take the sum of these weighted values. This is analogous to multiple linear regression, where the weights correspond to simple coefficients. \n",
    "\n",
    "![ann9.png](ann9.png)\n",
    "\n",
    "However, now assume we have a pretrained neural network with a single hidden layer of five neurons. There are synaptic connections linking every node in adjacent layers. However, the weights on certain neurons will be negligible, such that, in practice, nodes in the hidden layer respond only to a subset of input variables.\n",
    "\n",
    "![ann10.png](ann10.png)\n",
    "\n",
    "For example, the first hidden neuron may only respond to $x_1$ and $x_3$, the second to $x_2$ and $x_3$, the third to $x_1$, $x_2$ and $x_4$ the fourth to all inputs and the fifth to $x_4$. These nodes have picked up on particular features of the data. We may theorize on the nature of these linked features using our own human intuition and reasoning for this particular case study.\n",
    "\n",
    "## How do Neural Networks Learn?\n",
    "\n",
    "The Neuron, in isolation, is the simplest type of neural network. It is a single layer feed-forward NN, or a perceptron. Now we speak of training, the actual measured value is denoted by $y$, and $\\hat{y}$ is the predicted value.\n",
    "\n",
    "To begin with we look at how training works for a single row of input data. We start with some initial configuration of weights and calculate $\\hat{y}$. When $\\hat{y}$ is calculated, we calculate the cost function $C(\\hat{y}, y)$. We can choose from a number of cost functions, but the most simple is half the squared difference of $y$ and $\\hat{y}$,\n",
    "\n",
    "\\begin{equation}\n",
    "    C = \\frac{1}{2}(\\hat{y} - y)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Once $C$ has been calculated, the result is fed back to adjust the weights, with the aim of minimising the cost function through successive iterations.\n",
    "\n",
    "![ann11.png](ann11.png)\n",
    "\n",
    "Now let's extend the discussion to multiple rows. There are multiple epochs to training an NN. In the first epoch, we calculate $\\hat{y}$ for each row in succession. Then once all rows are calculated we calculate the cost function,\n",
    "\n",
    "\\begin{equation}\n",
    "    C = \\frac{1}{2}\\sum_i(\\hat{y}_i - y_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "This result is then fed back in to the NN to adjust the weights, i.e. all the rows share the same weights, we're not dealing with a different NN for each row. \n",
    "\n",
    "![ann12.png](ann12.png)\n",
    "\n",
    "Now we begin the reiterate the process with the adjusted weights, for all rows, until we've minimised $C$. This process is called back propagation.\n",
    "\n",
    "[Link to further reading on different cost functions](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
